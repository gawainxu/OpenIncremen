from __future__ import print_function

import pickle
import os
import sys
import argparse
import time
import math

import torch
import torch.backends.cudnn as cudnn
from torchvision import transforms, datasets

from util import AverageMeter
from util import adjust_learning_rate, warmup_learning_rate, accuracy
from util import set_optimizer
from resnet_big import SupConResNet, LinearClassifier
from exemplars import createExemplars
from dataset import customDataset
from data_loader import iCIFAR10, iCIFAR100, mnist, TinyImagenet, apply_transform

try:
    import apex
    from apex import amp, optimizers
except ImportError:
    pass


def parse_option():
    parser = argparse.ArgumentParser('argument for training')

    parser.add_argument('--print_freq', type=int, default=10,
                        help='print frequency')
    parser.add_argument('--save_freq', type=int, default=10,
                        help='save frequency')
    parser.add_argument('--batch_size', type=int, default=128,
                        help='batch_size')
    parser.add_argument('--num_workers', type=int, default=16,
                        help='num of workers to use')
    parser.add_argument('--epochs', type=int, default=10,
                        help='number of training epochs')

    # optimization
    parser.add_argument('--learning_rate', type=float, default=0.1,
                        help='learning rate')
    parser.add_argument('--lr_decay_epochs', type=str, default='60,75,90',
                        help='where to decay lr, can be a list')
    parser.add_argument('--lr_decay_rate', type=float, default=0.2,
                        help='decay rate for learning rate')
    parser.add_argument('--weight_decay', type=float, default=0,
                        help='weight decay')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='momentum')
    parser.add_argument('--num_init_classes', type=int, default=90,
                        help='num of old classes')
    parser.add_argument('--num_classes', type=int, default=100,
                        help='number of training classes')

    # model dataset
    parser.add_argument('--model', type=str, default='resnet18')
    parser.add_argument('--dataset', type=str, default='cifar100',
                        choices=['cifar10', 'cifar100'], help='dataset')
    parser.add_argument('--method', type=str, default='SupCon',
                        choices=['SupCon', 'SimCLR'], help='choose method')

    # other setting
    parser.add_argument('--cosine', action='store_true',
                        help='using cosine annealing')
    parser.add_argument('--warm', action='store_true',
                        help='warm-up for large batch training')

    parser.add_argument('--ckpt', type=str, default='',
                        help='path to pre-trained model')

    opt = parser.parse_args()

    # set the path according to the environment
    opt.data_folder = './datasets/'

    iterations = opt.lr_decay_epochs.split(',')
    opt.lr_decay_epochs = list([])
    for it in iterations:
        opt.lr_decay_epochs.append(int(it))

    opt.model_path = './save/'
    opt.model_name = '{}_{}_class_{}_{}_lr_0.001_epoch_600_bsz_512_temp_0.05_alfa_0.2_mem_2000_incremental/last.pth'.\
                         format(opt.method, opt.dataset, opt.num_classes, opt.model)
                         
    opt.exemplar_file = './exemplars/exemplar_{}_class_{}_{}_memorysize_50_alfa_0.2_temp_0.05_mem_2000'.format(opt.dataset, opt.num_init_classes, opt.model)
    opt.save_path = "./save/Linear_{}_{}_class_{}_mem_2000.pt".format(opt.method, opt.dataset, opt.num_classes)

    return opt


def set_model(opt):
    model = SupConResNet(name=opt.model)
    criterion = torch.nn.CrossEntropyLoss()

    classifier = LinearClassifier(name=opt.model, num_classes=opt.num_classes)

    ckpt = torch.load(os.path.join(opt.model_path, opt.model_name), map_location='cpu')
    state_dict = ckpt['model']

    if torch.cuda.is_available():
        if torch.cuda.device_count() > 1:
            model.encoder = model.encoder #torch.nn.DataParallel(model.encoder)
        else:
            new_state_dict = {}
            for k, v in state_dict.items():
                k = k.replace("module.", "")
                new_state_dict[k] = v
            state_dict = new_state_dict
        model = model.cuda()
        classifier = classifier.cuda()
        criterion = criterion.cuda()
        cudnn.benchmark = True

        model.load_state_dict(state_dict)

    return model, classifier, criterion


def set_loader(opt):
    # construct data loader
    if opt.dataset == 'cifar10':
        mean = (0.4914, 0.4822, 0.4465)
        std = (0.2023, 0.1994, 0.2010)
    elif opt.dataset == 'cifar100':
        mean = (0.5071, 0.4867, 0.4408)
        std = (0.2675, 0.2565, 0.2761)
    elif opt.dataset == 'tinyimgnet':
        mean = (0.485, 0.456, 0.406)
        std = (0.229, 0.224, 0.225)
    elif opt.dataset == 'mnist':
        mean = (0.1307,)
        std = (0.3081,)
    elif opt.dataset == 'path':
        mean = eval(opt.mean)
        std = eval(opt.std)
    else:
        raise ValueError('dataset not supported: {}'.format(opt.dataset))
    normalize = transforms.Normalize(mean=mean, std=std)

    train_transform = transforms.Compose([
        #transforms.RandomResizedCrop(size=opt.size, scale=(0.2, 1.)),
        #transforms.RandomHorizontalFlip(),
        #transforms.RandomApply([
        #    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
        #], p=0.8),
        #transforms.RandomGrayscale(p=0.2),
        transforms.ToTensor(),
        normalize,
    ])

    if opt.dataset == 'cifar10':
        train_dataset = iCIFAR10(root='../datasets', train=True,
                                 classes=range(opt.num_init_classes, opt.num_classes), download=True,
                                 transform=None)
        original_dataset = iCIFAR10(root='../datasets', train=True,
                                    classes=range(0, opt.num_init_classes), download=True,
                                    transform=None)
        
    elif opt.dataset == 'cifar100':
        train_dataset = iCIFAR100(root='../datasets', train=True,
                                  classes=range(opt.num_init_classes, opt.num_classes), download=True,                 
                                  transform=None)                      
        original_dataset = iCIFAR100(root='../datasets', train=True,
                                     classes=range(0, opt.num_init_classes), download=True,
                                     transform=None)
        
    elif opt.dataset == "mnist":
        train_dataset = mnist(root='../datasets', train=True,
                              classes=range(opt.num_init_classes, opt.num_classes), download=True,
                              transform=None)
        original_dataset = mnist(root='../datasets', train=True,
                                 classes=range(0, opt.num_init_classes), 
                                 download=True, transform=None)
        
    elif opt.dataset == 'tinyimgnet':
        train_dataset = TinyImagenet(root='../datasets', train=True,
                                     classes=range(opt.num_init_classes, opt.num_classes), 
                                     download=True, transform=None)
        original_dataset = TinyImagenet(root='../datasets', train=True,
                                        classes=range(0, opt.num_init_classes), 
                                        download=True, transform=None)
        
    else:
        raise ValueError(opt.dataset)
        
    if os.path.isfile(opt.exemplar_file):
        with open(opt.exemplar_file, "rb") as f:
                    exemplar_sets, exemplar_labels, _, exemplar_centers = pickle.load(f, encoding='latin1')
    else:
        #transform = transforms.Compose([transforms.ToTensor(), normalize])      # TODO what kind of transform to use for exemplar selection?
        exemplar_sets, exemplar_labels, exemplar_centers = createExemplars(opt, original_dataset)
    #print(exemplar_sets.shape)
        
    exemplar_dataset = customDataset(exemplar_sets, exemplar_labels, transform=None)
    train_dataset = torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset])                               
    train_dataset = apply_transform(train_dataset, train_transform)
    print(len(train_dataset))

    train_sampler = None
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=(train_sampler is None),
                                               num_workers=opt.num_workers, pin_memory=True, sampler=train_sampler)

    return train_loader



def train(train_loader, model, classifier, criterion, optimizer, epoch, opt):
    """one epoch training"""
    model.eval()
    classifier.train()

    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    end = time.time()
    for idx, (images, labels) in enumerate(train_loader):
        data_time.update(time.time() - end)

        images = images.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        bsz = labels.shape[0]

        # warm-up learning rate
        #warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)

        # compute loss
        with torch.no_grad():
            features = model.encoder(images)
        output = classifier(features.detach())
        loss = criterion(output, labels)

        # update metric
        losses.update(loss.item(), bsz)
        acc, ueq= accuracy(output, labels)
        top1.update(acc, bsz)

        # SGD
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        # print info
        if (idx + 1) % opt.print_freq == 0:
            print('Train: [{0}][{1}/{2}]\t'
                  'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
                  'loss {loss.val:.3f} ({loss.avg:.3f})\t'
                  'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                   epoch, idx + 1, len(train_loader), batch_time=batch_time,
                   data_time=data_time, loss=losses, top1=top1))
            sys.stdout.flush()

    return losses.avg, top1.avg


def validate(val_loader, model, classifier, criterion, opt):
    """validation"""
    model.eval()
    classifier.eval()

    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    with torch.no_grad():
        end = time.time()
        for idx, (images, labels) in enumerate(val_loader):
            images = images.float().cuda()
            labels = labels.cuda()
            bsz = labels.shape[0]

            # forward
            output = classifier(model.encoder(images))
            loss = criterion(output, labels)

            # update metric
            losses.update(loss.item(), bsz)
            acc1, acc5 = accuracy(output, labels, topk=(1, 5))
            top1.update(acc1[0], bsz)

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            if idx % opt.print_freq == 0:
                print('Test: [{0}/{1}]\t'
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                      'Acc@1 {top1.val:.3f} ({top1.avg:.3f})'.format(
                       idx, len(val_loader), batch_time=batch_time,
                       loss=losses, top1=top1))

    print(' * Acc@1 {top1.avg:.3f}'.format(top1=top1))
    return losses.avg, top1.avg


def main():
    best_acc = 0
    opt = parse_option()

    # build data loader
    train_loader = set_loader(opt)

    # build model and criterion
    model, classifier, criterion = set_model(opt)

    # build optimizer
    optimizer = set_optimizer(opt, classifier)

    # training routine
    for epoch in range(1, opt.epochs + 1):
        #adjust_learning_rate(opt, optimizer, epoch)

        # train for one epoch
        time1 = time.time()
        loss, acc = train(train_loader, model, classifier, criterion,
                          optimizer, epoch, opt)
        time2 = time.time()
        print('Train epoch {}, total time {:.2f}, accuracy:{:.2f}'.format(
            epoch, time2 - time1, acc))

        # eval for one epoch
        #loss, val_acc = validate(val_loader, model, classifier, criterion, opt)
        #if val_acc > best_acc:
        #    best_acc = val_acc
    torch.save(classifier.state_dict(), opt.save_path)
    print('best accuracy: {:.2f}'.format(best_acc))


if __name__ == '__main__':
    main()
